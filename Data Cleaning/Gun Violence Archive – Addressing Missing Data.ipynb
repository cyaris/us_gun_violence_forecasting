{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "import glob\n",
    "import re\n",
    "from proxy_requests import ProxyRequests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run get_incident_file_list function\n",
    "# to output all recorded possible indident ids into one list\n",
    "from my_functions import get_incident_file_list\n",
    "# run get_incident_std_list function\n",
    "# to output possible indident ids from scraping methods into one list\n",
    "from my_functions import get_incident_std_list\n",
    "# run get_logged_incident_ids_list function\n",
    "# to output all logged indident ids into one list\n",
    "from my_functions import get_logged_incident_ids_list\n",
    "# run get_original_incident_ids_list function\n",
    "# to output all indident ids from original file into one list\n",
    "from my_functions import get_original_incident_ids_list\n",
    "# run get_id_status_queue function\n",
    "# to output all potential ids\n",
    "# that have not yet been logged\n",
    "# up to an inputted maximum id\n",
    "# minimum id has been set as site minimum\n",
    "from my_functions import get_id_status_queue\n",
    "# function outputs a dataframe of all manually requested status codes\n",
    "# that have been confirmed as 200 or 404\n",
    "from my_functions import get_status_code_df\n",
    "# function takes a list input to catalogue in the incident directory without duplicates\n",
    "# when requests are made later, they will only be for non-catalogued IDs\n",
    "# removing any potential id that is already in the incident folder\n",
    "from my_functions import get_id_file_add_queue\n",
    "# function to scrape whatismybrowser.com and return a popular header at random\n",
    "from my_functions import get_popular_header\n",
    "# function takes in a url as an input and assigns a random proxy and a random header\n",
    "# function returns status_code as 0 indice\n",
    "# function returns get_raw as 1 indice\n",
    "from my_functions import request_with_random_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_file_add_queue: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incident_std_list = get_incident_std_list()\n",
    "logged_ids_list = get_logged_incident_ids_list()\n",
    "# inputting ids from logged_ids_list and incident_std_list\n",
    "# that are not already in the incident directory into incident directory into the incident directory\n",
    "# making sure every catalogued (scraped and downloaded) ID is in the incident folder,\n",
    "# all scraped ids and all logged ids will be accounted for\n",
    "get_id_file_add_queue(logged_ids_list + incident_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a while loop to obtain the maximum_id to input into status_id function\n",
    "# status_code = None\n",
    "# while status_code!=200:\n",
    "#     request = request_with_random_header('https://www.gunviolencearchive.org/last-72-hours')\n",
    "#     status_code = request.get_status_code()\n",
    "#     print(str(status_code) + ', ' + str(request.get_proxy_used()))\n",
    "# raw_html = request.get_raw()\n",
    "# maximum_id = int(str(str(raw_html).split('<a href=\"/incident/')[1]).split('>')[0][:-1])\n",
    "# print('maximum_id: ' + str(maximum_id) + '\\n')\n",
    "# # looping through all unaccounted for potential ids to get remaining status codes\n",
    "# # retrieving status codes for all potential ids that have not yet been logged\n",
    "# # 200 codes will be sent to incident directory, and then to scraper later\n",
    "# id_status_queue = get_id_status_queue(maximum_id)\n",
    "# for possible_id in id_status_queue:\n",
    "#     request = request_with_random_header('https://www.gunviolencearchive.org/incident/' + str(possible_id))\n",
    "#     id_status_code = request.get_status_code()\n",
    "#     with open('../Data Sources/brute_force_id_status_codes.txt', 'a+') as f:\n",
    "#         f.write(\"\\n\" + str(possible_id) + ', ' + str(id_status_code))\n",
    "# #         print(str(possible_id) + ', ' + str(id_status_code) + ', ' + str(request.get_proxy_used()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_file_add_queue: 2963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_code_df = get_status_code_df()\n",
    "# make a new add_list for urls that have received 200 codes above\n",
    "status_200_ids = list(status_code_df[status_code_df['status_code']==200]['id'].unique())\n",
    "# inputting status_200_ids into incident directory\n",
    "# no status 200 id should already be in the incident directory\n",
    "# making sure every manually requested id that was successful\n",
    "# is in the incident folder\n",
    "get_id_file_add_queue(status_200_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_scrape_queue: 18000\n"
     ]
    }
   ],
   "source": [
    "incident_file_list = get_incident_file_list()\n",
    "original_ids_list = get_original_incident_ids_list()\n",
    "# adding all ids from incident file list that are not in the original ids list into the scraper queue\n",
    "# ids that have already been scraped are ok because of cache\n",
    "id_scrape_queue = deepcopy(list(set(incident_file_list) - set(original_ids_list)))\n",
    "\n",
    "with open('../Pickles/incident_id_scrape_queue.pkl', 'wb') as f:\n",
    "    pkl.dump(id_scrape_queue, f)\n",
    "\n",
    "print('id_scrape_queue: ' + str(len(id_scrape_queue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_file_list = get_incident_file_list()\n",
    "# catalogue all possible incident_ids into a csv file\n",
    "df = pd.DataFrame(incident_file_list, columns = ['possible_incident_ids.csv'])\n",
    "df.to_csv('../Data Sources/possible_incident_ids.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
